{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Augmented Generation (RAG) solves this by retrieving relevant data and injecting it into the prompt.\n",
        "\n",
        "In this notebook, we will learn:\n",
        "\n",
        "Embeddings: Representing text as vectors.\n",
        "Vector Stores: Storing and searching vectors (FAISS).\n",
        "Naïve RAG: The standard Retrieval -> Augment -> Generate pipeline.\n",
        "Indexing Challenges: Deep dive into how vector databases search efficiently (Flat, IVF, HNSW, PQ)."
      ],
      "metadata": {
        "id": "6UEpRWj1P7vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1:\n",
        "Embeddings:\n",
        "An embedding is a translation from Words to Lists of Numbers (Vectors), such that similar words represent close numbers.\n",
        "\n",
        "The Process (Flowchart)\n",
        "graph LR\n",
        "    A[\"Input Text ('Apple')\"] -->|Tokenization| B[\"Tokens (101, 255)\"]\n",
        "    B -->|Embedding Model| C[\"Vector List ([0.1, -0.5, 0.9...])\"]\n",
        "    C -->|Store| D[\"Vector Database\"]"
      ],
      "metadata": {
        "id": "kUKVLmLxQAOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B9-JGApTPY3Y"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-huggingface sentence-transformers langchain-community\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a FREE, open-source model from Hugging Face\n",
        "# 'all-MiniLM-L6-v2' is small, fast, and very good for English.\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Viewing a Vector\n",
        "Let's see what the word \"Apple\" looks like to the machine.\n",
        "\n",
        "Conceptual Note: Dimensions\n",
        "The vector below has 384 dimensions (for MiniLM).\n",
        "\n",
        "Imagine a graph with X and Y axes (2 Dimensions). You can plot a point (x, y).\n",
        "Now imagine adding Z (3 Dimensions).\n",
        "Now imagine 384 axes.\n",
        "Each axis represents a feature (e.g., \"Is it a fruit?\", \"Is it red?\", \"Is it tech-related?\"). The numbers aren't random; they encode meaning."
      ],
      "metadata": {
        "id": "yu_Z96CQQkU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Apple\")\n",
        "\n",
        "print(f\"Dimensionality: {len(vector)}\")\n",
        "print(f\"First 5 numbers: {vector[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9AynO-tQk8x",
        "outputId": "625be650-7a35-489b-ec08-ace34da601e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensionality: 384\n",
            "First 5 numbers: [-0.006138487718999386, 0.03101177327334881, 0.06479360908269882, 0.01094149798154831, 0.005267191678285599]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 3:  Cosine Similarity\n",
        "How do we know if two vectors are close? We measure the Angle between them.\n",
        "\n",
        "Cosine Similarity Formula\n",
        "\n",
        "\n",
        "1.0: Arrows point in the Exact Same Direction (Identical).\n",
        "0.0: Arrows are Perpendicular (Unrelated).\n",
        "-1.0: Arrows point in Opposite Directions (Opposite).\n",
        "Experiment: Let's compare \"Cat\", \"Dog\", and \"Car\"."
      ],
      "metadata": {
        "id": "TViDXBvBQpKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "vec_cat = embeddings.embed_query(\"Cat\")\n",
        "vec_dog = embeddings.embed_query(\"Dog\")\n",
        "vec_car = embeddings.embed_query(\"Car\")\n",
        "\n",
        "print(f\"Cat vs Dog: {cosine_similarity(vec_cat, vec_dog):.4f}\")\n",
        "print(f\"Cat vs Car: {cosine_similarity(vec_cat, vec_car):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LecqLJLyQrGm",
        "outputId": "02254a3e-3a85-4ec3-9f6e-9a2bb6009dba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cat vs Dog: 0.6606\n",
            "Cat vs Car: 0.4633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis\n",
        "You should see that Cat & Dog score higher (e.g., ~0.8) than Cat & Car (e.g., ~0.3).\n",
        "This Mathematical Distance is the foundation of all Search engines and RAG systems.\n",
        "This is arguably the most important concept in modern AI."
      ],
      "metadata": {
        "id": "PRUvgAO4Qt-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unit 2 - Part 4b: Naive RAG Pipeline\n",
        "1. Introduction: The Open-Book Test\n",
        "RAG (Retrieval-Augmented Generation) is just an Open-Book Test architecture.\n",
        "\n",
        "Retrieval: Find the right page in the textbook.\n",
        "Generation: Write the answer using that page.\n",
        "The Pipeline (Flowchart)\n",
        "graph TD\n",
        "    User[User Question] --> Retriever[Retriever System]\n",
        "    Retriever -->|Search Database| Docs[Relevant Documents]\n",
        "    Docs --> Combiner[Prompt Template]\n",
        "    User --> Combiner\n",
        "    Combiner -->|Full Prompt w/ Context| LLM[Gemini Model]\n",
        "    LLM --> Answer[Final Answer]"
      ],
      "metadata": {
        "id": "YPnQ4rWCQuk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet faiss-cpu langchain-huggingface sentence-transformers langchain-community langchain-google-genai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Using the same free model as Part 4a\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMpz_OZjQw-8",
        "outputId": "22738823-8978-4cc9-9e15-bdb73d4325bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 2\n",
        "The \"Knowledge Base\" (Grounding)\n",
        "LLMs hallucinate because they rely on \"parametric memory\" (what they learned during training). RAG introduces \"non-parametric memory\" (external facts).\n",
        "\n",
        "Let's define some facts the LLM definitely does not know."
      ],
      "metadata": {
        "id": "syweJQDEQ-6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Ram's favorite food is Pizza with extra cheese.\"),\n",
        "    Document(page_content=\"The secret password to the lab is 'Blueberry'.\"),\n",
        "    Document(page_content=\"LangChain is a framework for developing applications powered by language models.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "hEG_WwsWQ_cl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Indexing ( Storing the knowledge)\n",
        "We use FAISS (Facebook AI Similarity Search) to store the embeddings. Think of FAISS as a super-fast librarian that organizes books by content, not title."
      ],
      "metadata": {
        "id": "CgEX5tGGRDZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "MEvNeZWERECt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The RAG Chain\n",
        "We use LCEL to stitch it together.\n",
        "\n",
        "Step 1: The retriever takes the question, converts it to numbers, and finds the closest document. Step 2: RunnablePassthrough holds the question. Step 3: The prompt combines them."
      ],
      "metadata": {
        "id": "i-SFPRxKRHlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"\n",
        "Answer based ONLY on the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = chain.invoke(\"What is the secret password?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRccMuN_RJrR",
        "outputId": "661512b0-5965-4fa2-ed05-4574fcb1caa9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret password to the lab is 'Blueberry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unit 2 - Part 4c: Deep Dive into Indexing Algorithms\n",
        "1. Introduction: The Scale Problem\n",
        "Comparing 1 vector against 10 vectors is fast. Comparing 1 vector against 100 Million vectors is slow.\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) was built to solve this.\n",
        "\n",
        "The Trade-off Triangle\n",
        "You can pick 2:\n",
        "\n",
        "Speed (Query time)\n",
        "Accuracy (Recall)\n",
        "Memory (RAM usage)\n",
        "We will explore algorithms that optimize different corners of this triangle.\n",
        "\n",
        "FAISS\n",
        "To find similar items:\n",
        "Compare query with ALL vectors → VERY SLOW\n",
        "\n",
        "With FAISS:\n",
        "Use smart indexing → VERY FAST search\n",
        "\n",
        "FAISS = Toolbox\n",
        "IVF, Flat, HNSW = Tools inside the toolbox\n",
        "FAISS\n",
        "| Data Type       | Converted to vector using |\n",
        "| --------------- | ------------------------- |\n",
        "| Text            | BERT                      |\n",
        "| Image           | CLIP                      |\n",
        "| Audio           | Wav2Vec                   |\n",
        "| Medical records | ML models                 |\n",
        "Raw Data → ML Model → Embeddings → FAISS Index → Fast Search\n",
        "\n",
        "| Inside FAISS | Type              |\n",
        "| ------------ | ----------------- |\n",
        "| IndexFlatL2  | Brute force       |\n",
        "| IVF          | Clustering based  |\n",
        "| HNSW         | Graph based       |\n",
        "| PQ           | Compression based |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SUWnKfh9RORa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Mock Data: 10,000 vectors of size 128\n",
        "d = 128\n",
        "nb = 10000\n",
        "xb = np.random.random((nb, d)).astype('float32')"
      ],
      "metadata": {
        "id": "XXVxCv75RQo_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Flat Index (Brute Force)\n",
        "Concept: Check every single item.\n",
        "\n",
        "Algo: IndexFlatL2\n",
        "Pros: 100% Accuracy (Gold Standard).\n",
        "Cons: Slow (O(N)). Unusable at 1M+ vectors."
      ],
      "metadata": {
        "id": "P0paUFcLRW_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(xb)\n",
        "print(f\"Flat Index contains {index.ntotal} vectors\")\n",
        "xq = np.random.random((1, d)).astype('float32')\n",
        "k = 5\n",
        "D, I = index.search(xq, k)\n",
        "\n",
        "print(\"Nearest vector indices:\", I)\n",
        "print(\"Distances:\", D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXRo3jbQRXr5",
        "outputId": "ce64765a-fe41-4052-ac31-3097c93a03ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat Index contains 10000 vectors\n",
            "Nearest vector indices: [[2566  483 5435 6119  859]]\n",
            "Distances: [[13.689927 14.276472 14.403429 14.410736 14.534609]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_0 = index.reconstruct(0)\n",
        "print(vector_0[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZMHzFBoRTXG",
        "outputId": "5ca06a95-ad79-4548-898d-60a61be6ebd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2580179  0.98567814 0.24375844 0.8135036  0.14280279 0.740962\n",
            " 0.70723194 0.3000329  0.33059973 0.9030029 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IVF (Inverted File Index)\n",
        "Concept: Clustering / Partitioning.\n",
        "\n",
        "Imagine looking for a book. Instead of checking every shelf, you go to the \"Sci-Fi\" section. Then you only search books in that section.\n",
        "\n",
        "How it works (Flowchart)\n",
        "graph TD\n",
        "    Data[All 1M Vectors] -->|Train| Clusters[1000 Cluster Centers (Centroids)]\n",
        "    Query[User Query] -->|Step 1| FindClosest[Find Closest Centroid]\n",
        "    FindClosest -->|Step 2| Search[Search ONLY vectors in that Cluster]\n",
        "Analogy: Voronoi Cells (Zip Codes). We only search the local zip code."
      ],
      "metadata": {
        "id": "QmpT04x3Rcm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlist = 100 # How many 'zip codes' (clusters) we want\n",
        "quantizer = faiss.IndexFlatL2(d) # The calculator for distance\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "\n",
        "# We MUST train it first so it learns where the clusters are\n",
        "index_ivf.train(xb)\n",
        "index_ivf.add(xb)"
      ],
      "metadata": {
        "id": "FjLUdNO8Re-K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Is index trained?\", index_ivf.is_trained)\n",
        "print(\"Total vectors in index:\", index_ivf.ntotal)\n",
        "print(\"Number of clusters (nlist):\", index_ivf.nlist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITIWE5-hRhAL",
        "outputId": "6658eeb4-f3fa-479f-852c-d44b18afd2eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is index trained? True\n",
            "Total vectors in index: 10000\n",
            "Number of clusters (nlist): 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_ivf.nprobe = 5   # search in 5 clusters\n",
        "\n",
        "xq = np.random.random((1, d)).astype('float32')\n",
        "D, I = index_ivf.search(xq, 5)\n",
        "\n",
        "print(\"Nearest indices:\", I)\n",
        "print(\"Distances:\", D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHVh4ofRRi8y",
        "outputId": "1b630508-3996-45c7-99cc-92f9c0d97278"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest indices: [[3214 3550  891 5483  846]]\n",
            "Distances: [[13.808525 13.945784 14.322535 14.37821  14.569209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_ivf.nprobe = 5   # search in 5 clusters\n",
        "\n",
        "xq = np.random.random((1, d)).astype('float32')\n",
        "D, I = index_ivf.search(xq, 5)\n",
        "\n",
        "print(\"Nearest indices:\", I)\n",
        "print(\"Distances:\", D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQadDA-ORlBe",
        "outputId": "9309c8a9-69e6-4e45-b3f1-b07b4feff0cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest indices: [[4227 3368 9378 5290 8348]]\n",
            "Distances: [[14.8670845 15.842625  15.96616   16.341606  16.519085 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. HNSW (Hierarchical Navigable Small World)\n",
        "Concept: Six Degrees of Separation.\n",
        "\n",
        "Most data is connected. HNSW builds a Graph.\n",
        "\n",
        "Layer 0: Every point connects to neighbors.\n",
        "Layer 1: \"Express Highways\" connecting distant points.\n",
        "Analogy: Catching a flight. You don't fly Local -> Local -> Local. You fly Local -> HUB (Chicago) -> HUB (London) -> Local.\n",
        "\n",
        "Pros: Extremely fast retrieval.\n",
        "Cons: Heavier on RAM (needs to store the edges of the graph)."
      ],
      "metadata": {
        "id": "XKRQD9aKRneo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = 16 # Number of connections per node (The 'Hub' factor)\n",
        "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
        "index_hnsw.add(xb)"
      ],
      "metadata": {
        "id": "edK8sW8VRoEy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xq = np.random.random((1, d)).astype('float32')\n",
        "\n",
        "D, I = index_hnsw.search(xq, 5)\n",
        "\n",
        "print(\"Nearest indices:\", I)\n",
        "print(\"Distances:\", D)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2R52mGPRq3Q",
        "outputId": "5c6967b3-4398-4c74-e8c1-05f5a251a24b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest indices: [[3722 4980 9386 7035 8119]]\n",
            "Distances: [[14.36725  15.214366 15.394323 15.525642 15.611689]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. PQ (Product Quantization)\n",
        "Concept: Compression (Lossy).\n",
        "\n",
        "Do we need 32-bit float precision (0.123456789)? No. 0.12 is fine. PQ breaks the vector into chunks and approximates them.\n",
        "\n",
        "Analogy: 4K Video vs 480p Video.\n",
        "\n",
        "480p is blurry, but it's 10x smaller and faster to stream.\n",
        "Use PQ when you are RAM constrained (e.g., storing 1 Billion vectors)"
      ],
      "metadata": {
        "id": "w4AVqnMARupa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 8 # Split vector into 8 sub-vectors\n",
        "index_pq = faiss.IndexPQ(d, m, 8)\n",
        "index_pq.train(xb)\n",
        "index_pq.add(xb)\n",
        "print(\"PQ Compression complete. RAM usage minimized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRpXlp9uRvJB",
        "outputId": "4c0b866f-25b0-40f5-9a40-6ea7afabd709"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PQ Compression complete. RAM usage minimized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Index | Speed     | Accuracy  | Memory   |\n",
        "| ----- | --------- | --------- | -------- |\n",
        "| Flat  | Slow      | 100%      | High     |\n",
        "| IVF   | Fast      | High      | Medium   |\n",
        "| HNSW  | Very Fast | Very High | High     |\n",
        "| PQ    | Very Fast | Medium    | Very Low |\n",
        "\n",
        "| Method | Think as        |\n",
        "| ------ | --------------- |\n",
        "| Flat   | Check All       |\n",
        "| IVF    | Go to Section   |\n",
        "| HNSW   | Travel via Hubs |\n",
        "| PQ     | Compress Data   |\n",
        "\n",
        "Flat → Exact but heavy\n",
        "IVF → Clustered search\n",
        "HNSW → Graph navigation\n",
        "PQ → Compressed storage\n",
        "\n"
      ],
      "metadata": {
        "id": "orSjB3fCRzwt"
      }
    }
  ]
}